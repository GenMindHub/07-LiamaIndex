{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook: AWS Bedrock with LlamaIndex and Elasticsearch\n",
    "\n",
    "This notebook sets up a project that integrates **AWS Bedrock**, **LlamaIndex**, and **Elasticsearch** for building a retrieval-augmented generation (RAG) application. The notebook provides step-by-step guidance on installation, setup, and implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone the Repository\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/GenMindHub/07-LiamaIndex.git\n",
    "cd 07-LiamaIndex\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Activate Virtual Environment\n",
    "\n",
    "```sh\n",
    "python3 -m venv .liamaindex\n",
    "source .liamaindex/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Jupyter Lab (Optional) \n",
    "\n",
    "If Jupyter Lab is not installed on your system, [install](https://jupyter.org/install) it using the following command:\n",
    "\n",
    "```sh\n",
    "python3 -m pip install -qU jupyterlab\n",
    "python3 -m pip install -qU notebook\n",
    "jupyter notebook password\n",
    "jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Install and Setup Elasticsearch\n",
    "\n",
    "Follow the instructions [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/run-elasticsearch-locally.html) to install and run Elasticsearch locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Install AWS CLI and Configure IAM Credentials\n",
    "\n",
    "Install AWS CLI using the guide [here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).\n",
    "\n",
    "Configure your AWS IAM credentials:\n",
    "\n",
    "```sh\n",
    "aws configure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "- `boto3` : it is an official AWS SDK for Python that allows you to interact with AWS services programmatically. \n",
    "- `python-dotenv` : Allows you to load environment variables from a .env file.Useful for storing API keys, AWS credentials, and other sensitive data without hardcoding them in your script.\n",
    "- `llama-index`: A framework for indexing and querying large-scale data using LLMs.Used for building retrieval-augmented generation (RAG) applications.\n",
    "- `llama-index-vector-stores-elasticsearch`: Elasticsearch vector store integration for LlamaIndex.Allows you to store and retrieve embeddings efficiently using Elasticsearch.\n",
    "- `llama-index-embeddings-bedrock`: AWS Bedrock embeddings integration for LlamaIndex.\n",
    "Enables you to generate text embeddings using AWS Bedrock foundation models.\n",
    "- `llama-index-llms-bedrock`: AWS Bedrock LLM integration for LlamaIndex.\n",
    "Allows you to use AWS-hosted foundation models for answering queries and processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "%pip install python-dotenv\n",
    "%pip install llama-index\n",
    "%pip install llama-index-vector-stores-elasticsearch\n",
    "%pip install llama-index-embeddings-bedrock\n",
    "%pip install llama-index-llms-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Creates a connection with AWS services\n",
    "import boto3\n",
    "# Loads AWS credentials and other environment variables from a .env file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Defines global configurations for LlamaIndex, such as embedding models and storage options.\n",
    "from llama_index.core import Settings\n",
    "# Manages persistent storage for vector embeddings and index metadata.\n",
    "from llama_index.core import StorageContext\n",
    "# Reads text files (e.g., PDFs, TXT, or JSON) from a local directory for processing\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "# Creates an index for storing document embeddings to enable efficient search\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Defines custom functions and query engines to enhance responses.\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Uses AWS Bedrock's foundation models (like Titan or Claude) to generate text embeddings.\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models\n",
    "# Stores and retrieves document embeddings using Elasticsearch as a vector database.\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "\n",
    "# Uses AWS Bedrock's LLMs for answering queries and processing text\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "# Enables a reasoning-based agent that interacts with users using LLM-powered decision-making.\n",
    "from llama_index.core.agent import ReActAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Parse PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pdf_folder: str):\n",
    "    reader = SimpleDirectoryReader(pdf_folder, required_exts=[\".pdf\"])\n",
    "    documents = reader.load_data()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize Elasticsearch as Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_elasticsearch():\n",
    "    host = os.getenv(\"ELASTIC_HOST\")\n",
    "    username = os.getenv(\"ELASTIC_USERNAME\")\n",
    "    password = os.getenv(\"ELASTIC_PASSWORD\")\n",
    "    index_name = os.getenv(\"INDEX_NAME\")\n",
    "    \n",
    "    vector_store = ElasticsearchStore(\n",
    "        index_name=index_name, es_url=host, es_user=username, es_password=password\n",
    "    )\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embed Documents and Store in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(documents, vector_store):\n",
    "    \n",
    "    # Initialize the bedrock embedding model \n",
    "    boto3_bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    bedrock_embedding_model = BedrockEmbedding(model_name=os.getenv(\"AWS_BEDROCK_EMBEDDING_MODEL\"), client=boto3_bedrock_client,)\n",
    "    # Set Embeddings Globally Using `settings`\n",
    "    Settings.embed_model = bedrock_embedding_model\n",
    "    # assign OpenSearch as the vector_store to the context\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # create index\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Basic Math Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_budget_tool(index, name, description):\n",
    "    query_engine = index.as_query_engine()\n",
    "    budget_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine,\n",
    "        name=name,\n",
    "        description=description,\n",
    "    )\n",
    "    return budget_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Initialize ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_react_agent(multiply_tool, add_tool, budget_tool):\n",
    "    agent = ReActAgent.from_tools(\n",
    "        [multiply_tool, add_tool, budget_tool], verbose=True\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "Settings.llm = Bedrock(model=os.getenv(\"AWS_BEDROCK_LLM_MODEL\"), temperature=0, max_tokens=1000)\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "documents = load_documents(\"./data\") # path is relative to virtual env\n",
    "print(documents)\n",
    "\n",
    "print(\"Initializing OpenSearch...\")\n",
    "vector_store = initialize_elasticsearch()\n",
    "print(vector_store)\n",
    "\n",
    "\n",
    "print(\"Creating index and storing embeddings...\")\n",
    "index = create_index(documents, vector_store)\n",
    "\n",
    "print(\"Get Basic Maths Functions and Query Engine Tools...\")\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "budget_tool = get_budget_tool(\n",
    "    index, \n",
    "    \"canadian_budget_2023\", \n",
    "    \"A RAG engine with some basic facts about the 2023 Canadian federal budget.\"\n",
    ")\n",
    "print(budget_tool)\n",
    "\n",
    "print(\"Initialize and Get ReAct Agent for Chat...\")\n",
    "agent = get_react_agent(multiply_tool, add_tool, budget_tool)\n",
    "response = agent.chat(\n",
    "    \"What is the total amount of the 2023 Canadian federal budget multiplied by 3? Go step by step, using a tool to do any math.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of workflow\n",
    "- Load environment variables.\n",
    "- Initialize AWS client for Bedrock.\n",
    "- Set up LlamaIndex settings (embedding model, storage).\n",
    "- Read documents from a directory.\n",
    "- Create vector embeddings using AWS Bedrock.\n",
    "- Store embeddings in Elasticsearch for retrieval.\n",
    "- Set up Bedrock LLM for answering queries.\n",
    "- Create tools and agents to enhance interactions.\n",
    "- Use the ReAct agent to process and respond to user queries dynamically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
